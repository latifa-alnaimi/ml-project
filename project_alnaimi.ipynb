{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project\n",
    "\n",
    "### Solution by: *Latifa Al-Naimi* \n",
    "\n",
    "## Description\n",
    "The project I've chosen to model through a supervised machine learning project is the prediction of tweets representing sentiments that were supportive or condemning towards public displays of feminism or women's rights activism. I focus on tweets that were responses to a woman declaring her liberation through twitter when she sought asylum after leaving an oppressive environment. She publicly disclosed her name and picture, which garnered wide array of responses. This is my training data. I use this data to build a model to classify supportive and condemning sentiments.\n",
    "\n",
    "### Motivation\n",
    "My long term goal is to understand in what ways anti-feminist sentiments manifest in the Middle East, in both English and Arabic. For the scope of this project, my goal was to build a simple predictor of anti-feminist sentiment by feature extraction. In the future, I aim to identify the most important features and test them on a variety of datasets from other contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection\n",
    "\n",
    "### 1.1 Tweepy setup\n",
    "I installed tweepy onto the conda environment created at the beginning of class: `conda install tweepy`. This is required to be able to use Twitter API v2 via Python. The `btoken` variable below represents the \"bearer token\" --a secret key that ties my personal application to a tweepy client. It has been removed prior to submission of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from langdetect import detect\n",
    "\n",
    "#from twitter_authentication import btoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authenticate this application\n",
    "btoken = \"redacted\"\n",
    "client = tweepy.Client(btoken, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Obtaining training data\n",
    "#### Context\n",
    "The training data I obtained is based on interactions in the time frame of a specific tweet posted by user @Aishalqahtani. The tweet is in Arabic, but in this tweet, Aisha disclosed her name and picture for the first time after years of being anonymous while living in Qatar. She posted this after seeking asylum in the UK and subsequently received a litany of responses in suppport of and criticizing her actions. The code below retrieves  metadata tied to that specific tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(data=<Tweet id=1218892822599950343 text=سلڤادور هي عائشة القحطاني، قطرية، محبة لوطني، ولكن أمضيت ٢٢ سنة تحت القوانين القامعة للمرأة في قطر، والتي تعطي كل التفويض لذكر العائلة، القوانين التي تدهس على المعنفات وتكرههم على التنازل عن حقوقهم، اقف هنا بعد ان نجيت بذاتي من كل ذلك، لأتحدث عن تجربتي وتجربة غيري من النساء. https://t.co/XT1urKj3jX>, includes={}, errors=[], meta={})\n"
     ]
    }
   ],
   "source": [
    "origin_id = \"1218892822599950343\"\n",
    "initial_data = client.get_tweet(origin_id, tweet_fields=['geo','referenced_tweets'])\n",
    "print(initial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweepy queries\n",
    "Using the time frame of the above tweet, I queried all tweets in that time frame that were replies or quote tweets of the user @aishalqahtani and stored the responses in `aisha_tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1q/n1dh88bn32vg_dkhg718dqnm0000gn/T/ipykernel_35091/1406439837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Querying replies and retweets to Aisha's 'tweet of independence'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maisha_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m for response in tweepy.Paginator(client.search_all_tweets, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                  \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'to:aishalqahtani OR retweets_of:aishalqahtani'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0muser_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'profile_image_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "# Querying replies and retweets to Aisha's 'tweet of independence'\n",
    "aisha_tweets = []\n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                 query = 'to:aishalqahtani OR retweets_of:aishalqahtani',\n",
    "                                 user_fields = ['username', 'name', 'location', 'profile_image_url'],\n",
    "                                 tweet_fields = ['id','created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                 expansions = 'author_id',\n",
    "                                 start_time = '2020-01-19T06:00:00Z',\n",
    "                                 end_time = '2020-01-30T00:00:00Z',\n",
    "                              max_results=500):\n",
    "    time.sleep(1)\n",
    "    aisha_tweets.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aisha_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1q/n1dh88bn32vg_dkhg718dqnm0000gn/T/ipykernel_35091/3452693604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example response tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maisha_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'aisha_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# Example response tweet\n",
    "aisha_tweets[10].data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting responses to dataframe\n",
    "Responses returned above are a complex multi-level data structure, so I created a function `to_dataframe()` so that it could be used for converting both training and test responses to data frames. The function body was obtained from this Twitter API v2 tutorial: <https://www.youtube.com/watch?v=rQEsIs9LERM>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_dataframe(responses: list): \n",
    "    result = []\n",
    "    user_dict = {}\n",
    "    # Loop through each response object\n",
    "    for response in responses:\n",
    "        # Take all of the users, and put them into a dictionary of dictionaries with the info we want to keep\n",
    "        for user in response.includes['users']:\n",
    "            user_dict[user.id] = {'username': user.username, \n",
    "                                  'name': user.name,\n",
    "                                  'profile_image_url': user.profile_image_url,\n",
    "                                  'location': user.location\n",
    "                                 }\n",
    "        for tweet in response.data:\n",
    "            # For each tweet, find the author's information\n",
    "            author_info = user_dict[tweet.author_id]\n",
    "            # Put all of the information we want to keep in a single dictionary for each tweet\n",
    "            result.append({'author_id': tweet.author_id, \n",
    "                           'username': author_info['username'],\n",
    "                           'name': author_info['name'],\n",
    "                           'author_location': author_info['location'],\n",
    "                           'text': tweet.text,\n",
    "                           'created_at': tweet.created_at,\n",
    "                           'retweets': tweet.public_metrics['retweet_count'],\n",
    "                           'replies': tweet.public_metrics['reply_count'],\n",
    "                           'likes': tweet.public_metrics['like_count'],\n",
    "                           'quote_count': tweet.public_metrics['quote_count']\n",
    "                          })\n",
    "\n",
    "    # Change this list of dictionaries into a dataframe\n",
    "    df = pd.DataFrame(result)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisha_reaction_df = to_dataframe(aisha_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>author_location</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>replies</th>\n",
       "      <th>likes</th>\n",
       "      <th>quote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1216698138125271041</td>\n",
       "      <td>Genat99952881</td>\n",
       "      <td>Genat</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @sa_vadorr: هناك فتيات يتعرضن للضرب والاضطه...</td>\n",
       "      <td>2020-01-29 23:04:47+00:00</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147036773</td>\n",
       "      <td>ArabianSaluki</td>\n",
       "      <td>Arabian Saluki</td>\n",
       "      <td>Riyadh, Saudi Arabia</td>\n",
       "      <td>RT @sa_vadorr: طبعا قناة الجزيرة حاليا على وضع...</td>\n",
       "      <td>2020-01-29 22:42:08+00:00</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147036773</td>\n",
       "      <td>ArabianSaluki</td>\n",
       "      <td>Arabian Saluki</td>\n",
       "      <td>Riyadh, Saudi Arabia</td>\n",
       "      <td>RT @sa_vadorr: يا عيني!، الجديد الان انه عندما...</td>\n",
       "      <td>2020-01-29 22:39:53+00:00</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17024104</td>\n",
       "      <td>innosinz</td>\n",
       "      <td>Nicole Deniese Harris</td>\n",
       "      <td>Virginia, USA</td>\n",
       "      <td>@sa_vadorr 100% right, I covered not knowing i...</td>\n",
       "      <td>2020-01-29 22:08:39+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>541397323</td>\n",
       "      <td>Critic_man</td>\n",
       "      <td>ناصر</td>\n",
       "      <td>None</td>\n",
       "      <td>RT @sa_vadorr: سلڤادور هي عائشة القحطاني، قطري...</td>\n",
       "      <td>2020-01-29 21:06:45+00:00</td>\n",
       "      <td>1735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6397</th>\n",
       "      <td>953150634865430528</td>\n",
       "      <td>naah1980</td>\n",
       "      <td>نــاهــد</td>\n",
       "      <td>None</td>\n",
       "      <td>@sa_vadorr ياشماتت ابله طازه فينا 😂😂😂</td>\n",
       "      <td>2020-01-19 13:31:49+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6398</th>\n",
       "      <td>2911674320</td>\n",
       "      <td>AlyafieHanan</td>\n",
       "      <td>❄ حنان اليافعي</td>\n",
       "      <td>خارجَ السّرْب</td>\n",
       "      <td>RT @sa_vadorr: جريدة الراية اثارت الرأي العام،...</td>\n",
       "      <td>2020-01-19 13:19:54+00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6399</th>\n",
       "      <td>1209870499284697088</td>\n",
       "      <td>keyalrumaihi</td>\n",
       "      <td>keyalrumaihi🇶🇦</td>\n",
       "      <td>Doha, Qatar</td>\n",
       "      <td>@sa_vadorr بعد اتذمون في البلاد ذم ولسانكم طوي...</td>\n",
       "      <td>2020-01-19 11:58:23+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>1086235403710054400</td>\n",
       "      <td>Ayshalkubaisi1</td>\n",
       "      <td>Ayshalkubaisi_</td>\n",
       "      <td>None</td>\n",
       "      <td>@sa_vadorr عيشي انسانيتچ الغير سوية مع نفسچ لا...</td>\n",
       "      <td>2020-01-19 11:46:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6401</th>\n",
       "      <td>1086235403710054400</td>\n",
       "      <td>Ayshalkubaisi1</td>\n",
       "      <td>Ayshalkubaisi_</td>\n",
       "      <td>None</td>\n",
       "      <td>@sa_vadorr الدعوة ترد لصاحبها ، وانتي شنو موقف...</td>\n",
       "      <td>2020-01-19 11:34:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6402 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author_id        username                   name  \\\n",
       "0     1216698138125271041   Genat99952881                  Genat   \n",
       "1               147036773   ArabianSaluki         Arabian Saluki   \n",
       "2               147036773   ArabianSaluki         Arabian Saluki   \n",
       "3                17024104        innosinz  Nicole Deniese Harris   \n",
       "4               541397323      Critic_man                   ناصر   \n",
       "...                   ...             ...                    ...   \n",
       "6397   953150634865430528        naah1980               نــاهــد   \n",
       "6398           2911674320    AlyafieHanan         ❄ حنان اليافعي   \n",
       "6399  1209870499284697088    keyalrumaihi         keyalrumaihi🇶🇦   \n",
       "6400  1086235403710054400  Ayshalkubaisi1         Ayshalkubaisi_   \n",
       "6401  1086235403710054400  Ayshalkubaisi1         Ayshalkubaisi_   \n",
       "\n",
       "           author_location                                               text  \\\n",
       "0                     None  RT @sa_vadorr: هناك فتيات يتعرضن للضرب والاضطه...   \n",
       "1     Riyadh, Saudi Arabia  RT @sa_vadorr: طبعا قناة الجزيرة حاليا على وضع...   \n",
       "2     Riyadh, Saudi Arabia  RT @sa_vadorr: يا عيني!، الجديد الان انه عندما...   \n",
       "3            Virginia, USA  @sa_vadorr 100% right, I covered not knowing i...   \n",
       "4                     None  RT @sa_vadorr: سلڤادور هي عائشة القحطاني، قطري...   \n",
       "...                    ...                                                ...   \n",
       "6397                  None              @sa_vadorr ياشماتت ابله طازه فينا 😂😂😂   \n",
       "6398         خارجَ السّرْب  RT @sa_vadorr: جريدة الراية اثارت الرأي العام،...   \n",
       "6399           Doha, Qatar  @sa_vadorr بعد اتذمون في البلاد ذم ولسانكم طوي...   \n",
       "6400                  None  @sa_vadorr عيشي انسانيتچ الغير سوية مع نفسچ لا...   \n",
       "6401                  None  @sa_vadorr الدعوة ترد لصاحبها ، وانتي شنو موقف...   \n",
       "\n",
       "                    created_at  retweets  replies  likes  quote_count  \n",
       "0    2020-01-29 23:04:47+00:00        95        0      0            0  \n",
       "1    2020-01-29 22:42:08+00:00       182        0      0            0  \n",
       "2    2020-01-29 22:39:53+00:00        77        0      0            0  \n",
       "3    2020-01-29 22:08:39+00:00         0        0      0            0  \n",
       "4    2020-01-29 21:06:45+00:00      1735        0      0            0  \n",
       "...                        ...       ...      ...    ...          ...  \n",
       "6397 2020-01-19 13:31:49+00:00         0        0      1            0  \n",
       "6398 2020-01-19 13:19:54+00:00        20        0      0            0  \n",
       "6399 2020-01-19 11:58:23+00:00         0        0      0            0  \n",
       "6400 2020-01-19 11:46:27+00:00         0        0      0            0  \n",
       "6401 2020-01-19 11:34:05+00:00         0        1      1            0  \n",
       "\n",
       "[6402 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aisha_reaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisha_reaction_df.to_csv('aisha_interactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Obtaining test data\n",
    "The method used to obtain test data is the same as training data, however, I chose a more recent incident with a different user, @noofalmaadeed, and obtained interactions within a shorter time frame (2-3 weeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "noof_tweets = []\n",
    "for response in tweepy.Paginator(client.search_all_tweets, \n",
    "                                 query = 'to:noofalmaadeed OR retweets_of:noofalmaadeed',\n",
    "                                 user_fields = ['username', 'name', 'location', 'profile_image_url'],\n",
    "                                 tweet_fields = ['id','created_at', 'geo', 'public_metrics', 'text'],\n",
    "                                 expansions = 'author_id',\n",
    "                                 start_time = '2021-10-11T00:00:00Z',\n",
    "                                 end_time = '2021-10-30T00:00:00Z',\n",
    "                              max_results=500):\n",
    "    time.sleep(1)\n",
    "    noof_tweets.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "noof_reaction_df = to_dataframe(noof_tweets)\n",
    "noof_reaction_df.to_csv('noof_interactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning\n",
    "### 2.1 Removing duplicate retweets\n",
    "There were several duplicate retweets listed as separate datasets, so I took measures to remove them using the drop_duplicates() function in `pandas`. I did this for both training and test and test CSV files.\n",
    "\n",
    "* Initial number of training tweets: 6401\n",
    "\n",
    "* Initial number of test tweets: 637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "df = pd.read_csv('training_duplicates.csv', \n",
    "                usecols=['text']).drop_duplicates(keep=False).reset_index()\n",
    "df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "df = pd.read_csv('test_duplicates.csv', \n",
    "                usecols=['text']).drop_duplicates(keep=False).reset_index()\n",
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Offline CSV restructuring\n",
    "When I retrieved the tweets using the twitter API client, I specified several meta data that were interesting to me and seemed like they might be relevant. However, this meta data was removed for this particular project as I am only analyzing the text data in the tweets for the time being. \n",
    "\n",
    "Steps taken offline:\n",
    "* Removed non-text data (e.g. user location, name, etc) as it is irrelevant to this project. \n",
    "* Removed the old index column\n",
    "* Removed header\n",
    "\n",
    "After steps **2.1** and **2.2**, I ended up with:\n",
    "* Training tweets: 3243\n",
    "* Test tweets: 341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Annotation\n",
    "For this project, annotation was a requirement for a number of reasons. First, there were no similar datasets that were labeled due to the niche nature of the project. Second, The tweets of interest were both in English and Arabic as both languages are used quite frequently in the geographic region I am studying. But more than that, the Arabic used is not classic Arabic, rather a specific dialect unique to that region, so correct annotations can only be administered by someone who understands them both. The alternative would be to cut out a significant portion of the dataset, but for this study (or an extension of it), both languages need to be taken into account.\n",
    "\n",
    "### 3.1 Label description\n",
    "Offline, I added a column for the label in both `training.csv` and `test.csv`.\n",
    "I used the following annotations to label tweets:\n",
    "* 1 for supportive sentiment\n",
    "* -1 for opposing or invalidating sentiment\n",
    "* 0 for irrelevant, neutral or ambiguous tweets \n",
    "\n",
    "### 3.2 Annotation results:\n",
    "The goal was to end up with $1000$ training data points, however, due to time constraints, I managed $816$ (negative and positive). Including zeroes, total annotations are $1515$.\n",
    "\n",
    "| Dataset  |  $1$  | $-1$  | $0$    |\n",
    "| -------- |-----|-----|------|\n",
    "| Training | $231$ | $585$ | $699$  |\n",
    "| Test     | $75$  | $65$  | $186$  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dataset refinement\n",
    "Due to the ambiguous nature of the zero-labeled tweets, I've decided to remove them from consideration (in the following code block). I also remove @mentions from the tweet contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100% right, I covered not knowing it was my c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>كمية طاقه سلبيه في ويهج وكلامج..مسكينه انتي و...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>كذب من قال إن التعري يزيد المرأة جمالا https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>الله لا يبلانا بس ويسترنا فالدنيا والاخرة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>(يَا أَيُّهَا النَّبِيُّ قُل لِّأَزْوَاجِكَ و...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>3237</td>\n",
       "      <td>1.0</td>\n",
       "      <td>you gorgeous mashallah ✨✨✨ thank you for shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>3238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>اما الان فأستطيع ان اكون صوت للواتي يردن ان يش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>3239</td>\n",
       "      <td>1.0</td>\n",
       "      <td>يجب ان يعي الجميع بكمية الحقوق المسلوبة من الم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>3242</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>عيشي انسانيتچ الغير سوية مع نفسچ لاتحرضين الب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>3243</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>الدعوة ترد لصاحبها ، وانتي شنو موقفچ يوم قالو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1                                                  2\n",
       "0        0  1.0   100% right, I covered not knowing it was my c...\n",
       "1        1 -1.0   كمية طاقه سلبيه في ويهج وكلامج..مسكينه انتي و...\n",
       "2        3 -1.0   كذب من قال إن التعري يزيد المرأة جمالا https:...\n",
       "3        4 -1.0          الله لا يبلانا بس ويسترنا فالدنيا والاخرة\n",
       "4        5 -1.0   (يَا أَيُّهَا النَّبِيُّ قُل لِّأَزْوَاجِكَ و...\n",
       "...    ...  ...                                                ...\n",
       "2515  3237  1.0   you gorgeous mashallah ✨✨✨ thank you for shar...\n",
       "2516  3238  1.0  اما الان فأستطيع ان اكون صوت للواتي يردن ان يش...\n",
       "2517  3239  1.0  يجب ان يعي الجميع بكمية الحقوق المسلوبة من الم...\n",
       "2518  3242 -1.0   عيشي انسانيتچ الغير سوية مع نفسچ لاتحرضين الب...\n",
       "2519  3243 -1.0   الدعوة ترد لصاحبها ، وانتي شنو موقفچ يوم قالو...\n",
       "\n",
       "[813 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('input-csv/train.csv', header=None)\n",
    "# remove nonlabeled rows\n",
    "train_df = train_df.dropna()  \n",
    "\n",
    "# remove rows with 0 labels \n",
    "train_df = train_df.loc[train_df[1] != 0] \n",
    "\n",
    "# remove mentions\n",
    "train_df[2] = train_df[2].replace(r'@.*?(?=\\s)', '', regex=True)\n",
    "\n",
    "train_df\n",
    "\n",
    "#train_df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>شلون وثقتي💔؟ ٪يارب انج بخير</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>وايد مفتلمه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>هل انتي بجير حنآ معاك وا نحبك كثير 💞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Still worried for u… #وين_نوف</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Stay safe remember be strong! We love u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>الله يحفظك وماعليك شر ان شاءالله انتي في بلد ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>اتمنى ان اللي يعرف نوف يتركها في حالها \\nتعيش...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>انت اقوى من ما تعتقدين يا نوف\\nرغم ما تتحفظين...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>مافي مكان مثالي الا بيت والديج\\nقري مكانك و ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>335</td>\n",
       "      <td>1.0</td>\n",
       "      <td>نتمنى ان الجهات المختصة تأخذ الموضوع بمحمل ال...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1                                                  2\n",
       "0      0  1.0                        شلون وثقتي💔؟ ٪يارب انج بخير\n",
       "2      2 -1.0                                        وايد مفتلمه\n",
       "3      3  1.0               هل انتي بجير حنآ معاك وا نحبك كثير 💞\n",
       "4      4  1.0                      Still worried for u… #وين_نوف\n",
       "11    11  1.0            Stay safe remember be strong! We love u\n",
       "..   ...  ...                                                ...\n",
       "319  320  1.0   الله يحفظك وماعليك شر ان شاءالله انتي في بلد ...\n",
       "322  323 -1.0   اتمنى ان اللي يعرف نوف يتركها في حالها \\nتعيش...\n",
       "323  331  1.0   انت اقوى من ما تعتقدين يا نوف\\nرغم ما تتحفظين...\n",
       "324  333 -1.0    مافي مكان مثالي الا بيت والديج\\nقري مكانك و ...\n",
       "325  335  1.0   نتمنى ان الجهات المختصة تأخذ الموضوع بمحمل ال...\n",
       "\n",
       "[138 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('input-csv/test.csv', header=None)\n",
    "\n",
    "# remove nonlabeled rows\n",
    "test_df = test_df.dropna()  \n",
    "\n",
    "# remove rows with 0 labels \n",
    "test_df = test_df.loc[test_df[1] != 0] \n",
    "\n",
    "# remove mentions\n",
    "test_df[2] = test_df[2].replace(r'@.*?(?=\\s)', '', regex=True)\n",
    "\n",
    "test_df\n",
    "\n",
    "#test_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature extraction\n",
    "In this section, first I extract the labels from training and test: `Y_train` and `Y_test`, respectively. I also extract the tweet text data for both datasets, stored in `text_train` and `text_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_df.iloc[0:, 1].values\n",
    "text_train = train_df.iloc[0:, 2].values\n",
    "\n",
    "Y_test = test_df.iloc[0:, 1].values\n",
    "text_test = test_df.iloc[0:, 2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature prep function\n",
    "Here, I created a feature 'cleaning' function in order to remove unwanted text data. These are:\n",
    "* English and Arabic punctuation\n",
    "* English and Arabic stop words (from NLTK corpus)\n",
    "* Punctuation\n",
    "* Mentions, hashtags, and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "stopwords_list = stopwords.words('arabic')\n",
    "stopwords_list += stopwords.words('english')\n",
    "\n",
    "def remove_punctuations(text: str):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def preprocess(text: str):\n",
    "    text = text.lower()   \n",
    "     #Convert www.* or https?://* to \" \"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
    "    \n",
    "    #Replace #word with word\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "    # remove punctuations\n",
    "    text = remove_punctuations(text)\n",
    "    \n",
    "    \n",
    "    # remove repeated letters\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    \n",
    "    # remove arabic and english stop words\n",
    "    splt = text.split()\n",
    "    for i,word in enumerate(splt):\n",
    "        if word in stopwords_list:\n",
    "            splt[i]=splt[i].replace(word,'')\n",
    "            \n",
    "    text = ' '.join(splt)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ياعزتي  حسبنا الله ونعم الوكيل   غرر بدينك وفكرك الله يلطف بحالك وحال امك وأهلك واحبابك اتعبتيهم ذبحتهم وهم أحياء الله يلسط   ساعدك وغرر  عذاب الهون  الدنيا والآخرة'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing preprocessing function \n",
    "preprocess(text_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vectorization\n",
    "#### TF-IDF\n",
    "I decided to first test the TF-IDF vectorizer due to its nature of rating more important words appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_tf = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', ngram_range=(1, 2))\n",
    "X_train_tf = vect_tf.fit_transform(preprocess(tweet) for tweet in text_train)\n",
    "X_test_tf = vect_tf.transform(preprocess(tweet) for tweet in text_test)\n",
    "\n",
    "# print(vect.get_feature_names())\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "I wanted to compare the TF-IDF vectorization algorithm with a simpler implementation, the CountVectorizer, and see how that affects accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_ct = CountVectorizer(strip_accents='unicode', ngram_range=(1,2))\n",
    "X_train_ct = vect_ct.fit_transform(preprocess(tweet) for tweet in text_train)\n",
    "X_test_ct = vect_ct.transform(preprocess(tweet) for tweet in text_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classifier setup & hyperparameter tuning\n",
    "In this section, I perform hyperparameter tuning with 10-fold grid search cross-validation for three supervised ML classifiers: Logistic Regression, Support Vector Machines, and Random Forest. I included two `fit()` calls to test both of the vectorizers above. I uncommented one for each run to populate the table in **Section 5.4**.\n",
    "### 5.1 Model 1: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter settings: {'C': 1.0}\n",
      "Validation accuracy: 0.853613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#base_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', tol=1e-2, max_iter=500, random_state=123)\n",
    "base_classifier = LogisticRegression(penalty='l2', max_iter=1000, random_state=123)\n",
    "\n",
    "params = [{'C':[100, 10, 1.0, 0.1, 0.01]}]\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=10)\n",
    "gs_classifier.fit(X_train_ct, Y_train)\n",
    "#gs_classifier.fit(X_train_tf, Y_train)\n",
    "\n",
    "#print(gs_classifier.get_params())\n",
    "print(\"Best parameter settings:\", gs_classifier.best_params_)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual hyperparameter tuning\n",
    "Here, I demonstrate the effect of different values of `C` on the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "training_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for c in C:\n",
    "    classifier = LogisticRegression(C=c, penalty='l2', max_iter=1000, random_state=123)\n",
    "    classifier.fit(X_train_ct, Y_train) # X_train_ct: count vectorization\n",
    "    training_accuracies.append(accuracy_score(Y_train, classifier.predict(X_train_ct)))\n",
    "    test_accuracies.append(accuracy_score(Y_test, classifier.predict(X_test_ct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        C  training      test\n",
      "0  100.00  1.000000  0.782609\n",
      "1   10.00  1.000000  0.782609\n",
      "2    1.00  1.000000  0.775362\n",
      "3    0.10  0.986470  0.731884\n",
      "4    0.01  0.723247  0.471014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-73f9bf14780e4745b8f35ecde726e6bd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-73f9bf14780e4745b8f35ecde726e6bd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-73f9bf14780e4745b8f35ecde726e6bd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-ae4af7dfd232a2a7655e53526ba80cd2\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"dataset\"}, \"x\": {\"type\": \"nominal\", \"field\": \"C\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"accuracy\"}}, \"height\": 300, \"transform\": [{\"fold\": [\"training\", \"test\"], \"as\": [\"dataset\", \"accuracy\"]}], \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-ae4af7dfd232a2a7655e53526ba80cd2\": [{\"C\": 100.0, \"training\": 1.0, \"test\": 0.782608695652174}, {\"C\": 10.0, \"training\": 1.0, \"test\": 0.782608695652174}, {\"C\": 1.0, \"training\": 1.0, \"test\": 0.7753623188405797}, {\"C\": 0.1, \"training\": 0.986469864698647, \"test\": 0.7318840579710145}, {\"C\": 0.01, \"training\": 0.7232472324723247, \"test\": 0.47101449275362317}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy vs C value plot\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {'C': C,\n",
    "     'training': training_accuracies,\n",
    "     'test': test_accuracies \n",
    "    })\n",
    "print(data)\n",
    "\n",
    "\n",
    "alt.Chart(data).transform_fold(\n",
    "    ['training', 'test'],\n",
    "    as_=['dataset', 'accuracy']\n",
    ").mark_line(point=True).encode(\n",
    "    x='C:N',\n",
    "    y='accuracy:Q',\n",
    "    color='dataset:N'\n",
    ").properties(width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model 2: SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter settings: {'C': 100, 'kernel': 'linear'}\n",
      "Validation accuracy: 0.820431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "base_classifier = SVC(random_state=123)\n",
    "\n",
    "params = [{'kernel':['linear','rbf'], 'C': [100, 10, 1.0, 0.1, 0.01]}]\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=10)\n",
    "gs_classifier.fit(X_train_tf, Y_train)\n",
    "#gs_classifier.fit(X_train_ct, Y_train)\n",
    "\n",
    "print(\"Best parameter settings:\", gs_classifier.best_params_)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter settings: {'min_samples_split': 2, 'n_estimators': 300}\n",
      "Validation accuracy: 0.824074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "base_classifier = RandomForestClassifier(random_state=123)\n",
    "\n",
    "params = [{'n_estimators':[50, 100, 200, 300], 'min_samples_split': [2, 3, 6]}]\n",
    "\n",
    "gs_classifier = GridSearchCV(base_classifier, params, cv=10)\n",
    "#gs_classifier.fit(X_train_tf, Y_train)\n",
    "gs_classifier.fit(X_train_ct, Y_train)\n",
    "\n",
    "print(\"Best parameter settings:\", gs_classifier.best_params_)\n",
    "print(\"Validation accuracy: %0.6f\" % gs_classifier.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Tuning and cross-validation summary\n",
    "The table below demonstrates that the CountVectorizer produced higher validation accuracies.\n",
    "\n",
    "| Model              | Vectorizer       | Hyperparameters | Validation Accuracy | \n",
    "|--------------------| -----------------| ----------------| ---------------\n",
    "| logistic regression | TF-IDF          | `C`$=100$       | $0.805661$ | \n",
    "| logistic regression | CountVectorizer | `C`$=1.0$ | $0.853613$ |\n",
    "| SVM                 | TF-IDF          | `C`$=100$; `kernel=linear` | $0.820431$ |\n",
    "| SVM                 | CountVectorizer | `C`$=0.1$; `kernel=linear` | $0.841358$ |\n",
    "| random forest       | TF-IDF          | `n_estimators`$=300$; `min_samples_split`$=2$ | $0.820416$ | \n",
    "| random forest       | CountVectorizer | `n_estimators`$=300$; `min_samples_split`$=2$ | $0.824074$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the models\n",
    "Based on the information in the table above, I've decided to use only the `X_train` fit using CountVectorizer. Therefore, I'll be using the following optimal hyperparameters:\n",
    "* Logistic regression: `C`$=1.0$\n",
    "* SVM: `C`$=0.1$; `kernel=linear`\n",
    "* Random forest: `n_estimators`$=300$; `min_samples_split`$=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_ct\n",
    "X_test = X_test_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_acc = []\n",
    "\n",
    "# Logistic regression\n",
    "clf_lr = LogisticRegression(C=1.0, penalty='l2', max_iter=1000, random_state=123)\n",
    "clf_lr.fit(X_train, Y_train)\n",
    "\n",
    "test_acc.append(accuracy_score(Y_test, clf_lr.predict(X_test)))\n",
    "#clf_lr.score(X_test, Y_test)\n",
    "\n",
    "# SVM \n",
    "clf_svm = SVC(kernel='linear', C=0.1, random_state=123)\n",
    "clf_svm.fit(X_train, Y_train)\n",
    "\n",
    "test_acc.append(accuracy_score(Y_test, clf_svm.predict(X_test)))\n",
    "#clf_svm.score(X_test, Y_test)\n",
    "\n",
    "# Random forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=300, min_samples_split=2, random_state=123)\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "\n",
    "test_acc.append(accuracy_score(Y_test, clf_rf.predict(X_test)))\n",
    "#clf_rf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy\n",
      "0  logistic reg.  0.739130\n",
      "1            svm  0.731884\n",
      "2   rand. forest  0.710145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-03407cdf472d46fda2619839f4c54c2d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-03407cdf472d46fda2619839f4c54c2d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-03407cdf472d46fda2619839f4c54c2d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-59d6154ec80ccd546e68dce1a15a4921\"}, \"mark\": {\"type\": \"bar\", \"size\": 70}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"x\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Accuracy\", \"scale\": {\"domain\": [0.6, 0.8]}}}, \"height\": 300, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-59d6154ec80ccd546e68dce1a15a4921\": [{\"Model\": \"logistic reg.\", \"Accuracy\": 0.7391304347826086}, {\"Model\": \"svm\", \"Accuracy\": 0.7318840579710145}, {\"Model\": \"rand. forest\", \"Accuracy\": 0.7101449275362319}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'Model': ['logistic reg.', 'svm', 'rand. forest'],\n",
    "     'Accuracy': test_acc\n",
    "    })\n",
    "print(data)\n",
    "\n",
    "\n",
    "alt.Chart(data).mark_bar(size=70).encode(\n",
    "    alt.Y('Accuracy',\n",
    "         scale=alt.Scale(domain=[0.6, 0.80])),\n",
    "    x='Model:N',\n",
    "    color='Model:N'\n",
    ").properties(width=500, height=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initial evaluation\n",
    "### 7.1 Confusion matrix\n",
    "Due to class imbalance that has occurred while annotating, I suspect that the test accuracy might be inflated. As there are many more -1 labels in the training data, I believe that the models are predicting -1 most of the time. While that might be correct most of the time, the training dataset may not be training the models appropriately. To find out, I created a confusion matrix to evaluate false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fde772f2cd0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true=Y_test, y_pred=clf_lr.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_lr.classes_)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.66      0.92      0.77        65\n",
      "         1.0       0.89      0.58      0.70        73\n",
      "\n",
      "    accuracy                           0.74       138\n",
      "   macro avg       0.78      0.75      0.73       138\n",
      "weighted avg       0.78      0.74      0.73       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, clf_lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix indicates that there are more true positives and true negatives, which is a good indication, however, it is also predicting many false negatives (31). This means that the concern discussed above is valid. To fix this, there are a couple of techniques that can be employed: undersampling or oversampling. Since my training dataset is quite small, I've decided to attempt oversampling, rather than removing data (undersampling). This is explored in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Oversampling\n",
    "### 8.1 SMOTE\n",
    "In this section, I am using the Synthetic Minority Oversampling Technique (SMOTE) to balance the training data. SMOTE synthesizes new examples from the minority class (1.0 - supportive tweets). It does so by \"selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line\" [[reference]](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/). I use the `imblearn` python library to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling, counts of label '1': 228\n",
      "Before oversampling, counts of label '-1': 585\n",
      "After oversampling, counts of label '1': 585\n",
      "After oversampling, counts of label '-1': 585\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(f\"Before oversampling, counts of label '1': {sum(Y_train == 1)}\")\n",
    "print(f\"Before oversampling, counts of label '-1': {sum(Y_train == -1)}\")\n",
    "\n",
    "smote = SMOTE(random_state = 123)\n",
    "X_train_res, Y_train_res = smote.fit_resample(X_train, Y_train.ravel())\n",
    "\n",
    "print(f\"After oversampling, counts of label '1': {sum(Y_train_res == 1)}\")\n",
    "print(f\"After oversampling, counts of label '-1': {sum(Y_train_res == -1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Refitting & replotting \n",
    "Copied classifier fit codes from **Section 6**, but replacing `X_train` variable with the newly oversampled `X_train_res` (same for Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "\n",
    "# Logistic regression\n",
    "clf_lr = LogisticRegression(C=1.0, penalty='l2', max_iter=1000, random_state=123)\n",
    "clf_lr.fit(X_train_res, Y_train_res)\n",
    "\n",
    "test_acc.append(accuracy_score(Y_test, clf_lr.predict(X_test)))\n",
    "\n",
    "# SVM \n",
    "clf_svm = SVC(kernel='linear', C=0.1, random_state=123)\n",
    "clf_svm.fit(X_train_res, Y_train_res)\n",
    "\n",
    "test_acc.append(accuracy_score(Y_test, clf_svm.predict(X_test)))\n",
    "\n",
    "# Random forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=300, min_samples_split=2, random_state=123)\n",
    "clf_rf.fit(X_train_res, Y_train_res)\n",
    "\n",
    "test_acc.append(accuracy_score(Y_test, clf_rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy\n",
      "0  logistic reg.  0.724638\n",
      "1            svm  0.688406\n",
      "2   rand. forest  0.695652\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-0d7b52e5b0994882baab9b66fe610a0b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0d7b52e5b0994882baab9b66fe610a0b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0d7b52e5b0994882baab9b66fe610a0b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0e38a6a80ef9ab27dd55f8e46e7eed11\"}, \"mark\": {\"type\": \"bar\", \"size\": 70}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"x\": {\"type\": \"nominal\", \"field\": \"Model\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Accuracy\", \"scale\": {\"domain\": [0.6, 0.8]}}}, \"height\": 300, \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0e38a6a80ef9ab27dd55f8e46e7eed11\": [{\"Model\": \"logistic reg.\", \"Accuracy\": 0.7246376811594203}, {\"Model\": \"svm\", \"Accuracy\": 0.6884057971014492}, {\"Model\": \"rand. forest\", \"Accuracy\": 0.6956521739130435}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'Model': ['logistic reg.', 'svm', 'rand. forest'],\n",
    "     'Accuracy': test_acc\n",
    "    })\n",
    "print(data)\n",
    "\n",
    "\n",
    "alt.Chart(data).mark_bar(size=70).encode(\n",
    "    alt.Y('Accuracy',\n",
    "         scale=alt.Scale(domain=[0.6, 0.80])),\n",
    "    x='Model:N',\n",
    "    color='Model:N'\n",
    ").properties(width=500, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "Test accuracies have decreased across the board, as would be expected, because it initially appeared to be good at predicting the majority class (-1), but was actually most likely only reporting  Also, after oversampling, the Random Forest classifier outperformed the SVM, but Logistic Regression remains the best, with a test accuracy of $0.724638$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Secondary evaluation\n",
    "After balancing the training dataset, I re-evaluated accuracies as well as the confusion matrix based on the highest performing classifier, logistic regression.\n",
    "### 9.1 Confusion matrix\n",
    "The confusion matrix below shows that the over-prediction of false negatives has been alleviated by the balancing of the training data. False positives have increased, but that may be due to the fact that features associated with positive tweets are lacking. While I was annotating, I noticed that positive tweets were mostly very short phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fde72cc45b0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=clf_lr.predict(X_test))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf_lr.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Baseline accuracy\n",
    "With a balanced dataset, a baseline accuracy can be calculated based on assigning labels randomly. As this is a binary classification problem, random labeling would result in roughlt an accuracy of $0.50$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Precision, Recall, and F1 score\n",
    "In the next code block I regenerate the evaluation report after refitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.83      0.52      0.64        65\n",
      "         1.0       0.68      0.90      0.78        73\n",
      "\n",
      "    accuracy                           0.72       138\n",
      "   macro avg       0.75      0.71      0.71       138\n",
      "weighted avg       0.75      0.72      0.71       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, clf_lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "The accuracy reported for the logistic regression classifier (the highest performing one) is higher than the established baseline accuracy if the test were randomly labeled ($0.72 > 0.5$).\n",
    "\n",
    "With regards to precision and recall, the following table displays precision and recall scores reported for the classifier before and after balancing the data.\n",
    "\n",
    "|                  | Class | Precision | Recall | \n",
    "|------------------|-------|-----------|--------|\n",
    "| Before balancing | -1.0  |    0.66   |  0.92  |\n",
    "|                  |  1.0  |    0.89   |  0.58  |\n",
    "| After balancing  | -1.0  |    0.83   |  0.52  |\n",
    "|                  |  1.0  |    0.68   |  0.90  |\n",
    "\n",
    "The precision score of class -1.0 increased after balancing the data. This means that the classifier is making less false positive predictions (i.e. predicting that the tweet was an 'opposing' tweet when it actually was not). This is a result of reducing bias towards the majority class when it was imbalanced. However, the precision score for class 1.0 went down after balancing. I attribute this to lack of good features that represent that class well.\n",
    "\n",
    "Before balancing, the recall score of class 1.0 was fairly low (0.58), but increased after balancing to 0.9. This indicates that before balancing, the number of positives that were correctly predicted was low; i.e. there were many other positives that were predicted as -1.0. This is consistent with the majority class bias. After balancing, the bias reduced heavily, leading to a sharp rise in recall for class 1.0, which tells us that positives in class 1.0 are being misidentified a lot less. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "There are a few areas of improvements as well as future extensions to this project. Firstly, a larger dataset is needed in order to obtain better features for NLP of supportive and opposing sentiments of female liberation. This is especially true for the supportive class (1.0), as there were a lot less features to extract from those tweets, whereas the opposing class (-1) had tweets that were much more verbose.\n",
    "\n",
    "I also think this type of ML problem would benefit from a more sophisticated classification system, like adding a neutral label. The annotation guideline would differ depending on what angle I would be aiming for. For instance, many of the tweets referenced religion and culture, so one potential study could revolve around how likely it is for these themes to emerge in feminist calls for liberation on social media. \n",
    "\n",
    "I would also reattempt hyperparameter tuning of the chosen classifiers with the balanced dataset.\n",
    "\n",
    "Finally, I believe this project can be extended further to gain insights on women's rights discourse in the middle east. At the beginning of this notebook, I'd retreived metadata that I didn't end up using for this part of the project, but can be included for future extensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
